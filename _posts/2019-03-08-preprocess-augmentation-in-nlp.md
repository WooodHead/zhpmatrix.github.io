---
layout: post
title: "[NLP]聊一聊，预处理和数据增强技术"
excerpt: "这篇博客梳理了NLP中英文的预处理方法和一些通用但是实现成本不高的数据增强思路。"
date: 2019-03-08 18:43:00
mathjax: true
---

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

在[基于margin-loss的句子相似度](https://github.com/zhpmatrix/margin-loss-for-sentence-similarity)这个项目中，为了验证想法，找不到开放数据集，因此自己从新浪爱问爬取了数据。自己爬的数据和学界开放的数据对比，数据显得非常脏。这里有三个含义：第一：数据不规范，比如用词，写作等；第二：中文文本中蕴含了特殊符号等；第三：数据偏见在真实场景下是确实存在的。这里是从数据质量的角度来看，结合具体的场景，可能会有各种影响数据质量的因素。因此，我们需要数据预处理，目的是提升数据质量；

此外，数据规模也是重要的。我们需要数据增强技术来提升数据规模。这是数据增强技术的表面结果，从深层次的角度来看，数据增强技术可以有效防止模型的过拟合，提升模型的鲁棒性。所谓"见多识广"。

因此，为了提升数据质量和数据规模，我们需要预处理技术和数据增强技术。

### 一.预处理技术

#### 1.中文

#### (1)分词

中文分词通常是基于词的中文NLP场景下的第一步，虽然目前的中文分词社区仍然对于一些中文语句的分词结果不理想，但是整体上是可用的。自己常用的分词工具是哈工大的LTP。

#### (2)停用词过滤

停用词是高频，但是无意义的词。在中英文环境中都存在停用词，但是停用词是否要去掉是一个有争议的问题。从理论上分析，去掉停用词，有助于减少VOC的大小，加快收敛，节省存储和计算时间；减少停用词对句子语义表示的影响，有助于提升模型效果。但是实际上，预估停用词对任务的影响是困难的。因此实际场景下，可以做两个版本分别对比。

#### (3)特殊符号清洗

标点符号和词都是符号，但是还要一类特殊符号，诸如笑脸，数字，运算符号等，在一些场景下，需要注意这类特殊符号的过滤。如果这类符号是低频的，那么通过停用词过滤，可以去掉一部分符号，但是如果不能过滤掉，需要显式地去过滤该类符号。在上文提到的项目中，就会遇到这种表情符号特别丰富的场景。

#### (4)任务依赖的预处理

在分类任务中，可以通过NER技术将待分类文本中出现的地点，组织和人名替换为\<place\>，\<org\>和\<person\>，减少OOV的可能，优点同上。比如，可以应用在[AI Challenger 2018的细粒度情感分类比赛](https://github.com/xueyouluo/fsauor2018)中。


#### 2.英文

#### (1)分词

英文句子以空格作为分割符，分割单词。但是直接通过空格分隔符会导致下述情况下的分词结果的不正确性。比如：

**You're the apple of my eye!**

分词后，**eye!**和**eye**就表示不同的单词了。直观上来看，似有不合理之处。因此，显然，要去掉符号。

#### (2)标点符号过滤

在(1)中，去掉标点符号之后，得到，

**You re the apple of my eye**

这样的话，又会遇到**it's**和**its**的冲突。

因此，好的英文分词要考虑上述两种情况，NLTK对于

**it's my dog!**

的分词结果是：

**\[\[it\] \['s\] \[my\] \[dog\] \[!\]\]**

这正是我们需要的。

#### (3)大小写统一

在英文场景下需要考虑，有些特殊场景下不适合进行大小写的统一。比如，BERT的预训练模型就提供了两个版本的，分别是统一后和不做统一的。

#### (4)标准化(stemming&lemmatization)

这个是英文场景所特有的，举例如下：

第一种情况：比如对名词eye，有eye和eyes两种形态；

第二种情况：比如对动词take，有token, taken, take三种形态。

通过stemming(词干分析)解决第一种；通过lemmatization(词元分析)解决第二种；

如果说分词可以认为是中文所特有的（不严格成立，英文也可以分词，但是英文分词的必要性显然不如中文分词），那么标准化就是英文所特有的。

#### (5)subword

为了进一步减少VOC的大小，在机器翻译任务中比较流行的subword技术。比如两个单词，分别是person和possion,则subword可以表示为\[\[pers\],\[on\],\[possi\]\]，具体由[语料和BPE算法](https://github.com/rsennrich/subword-nmt)决定。

### 二.数据增强

#### 1.简单数据增强

#### (1)同义词替换

同义词的获取可以基于词向量获取其他技术构建的词的量化表示，通过距离计算相似度得到。

#### (2)同义词随机插入

将句子中选定的词的同义词随机插入句子任何位置，插入之后，句子变长。

#### (3)随机交换词汇位置

交换后，句子长度不变。

#### (4)随机删除

#### 2.任务依赖的增强技术

#### (1)back-translation

反向翻译用于机器翻译，用于[中文文本纠错任务](https://liweinlp.com/?p=5000)。

#### 3.复杂技术

#### (1)使用预测语言模型做同义词替换

#### (2)作为平滑技术的数据加噪

#### (3)基于生成模型(VAE&GAN&Flow)进行数据生成

总结：虽然梳理了一些预处理和数据增强的技术，但是实际任务中的技术选取还是要以下游任务的提升为基准点。2018年有一篇文章做CV领域的自动数据增强技术的，基于强化学习。NLP领域或许也可以做一做呀。从工具使用上，在预处理阶段，综合体验NLTK和哈工大的LTP结合使用最棒。可能与自己不载入一个模型就会感觉不舒服有关吧。本来写这篇博客的目的是想写一个预处理和数据增强的模块的，但是现在感觉意义不是很大。

主要参考：

1.《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》

2.[中文对比英文自然语言处理NLP的区别综述](https://zhuanlan.zhihu.com/p/59838270)

3.[《Low Resource Text Classification with ULMFit and Backtranslation》](https://arxiv.org/abs/1903.09244)

在情感分类任务上，比较了两种数据增强技术，分别是Random Token Perturbations和Backtranslation，证明了Backtranslation的有效性。














