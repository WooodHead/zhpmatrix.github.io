---
layout: post
title: "[ML&DM]GBDT-下里巴人版"
excerpt: "XGBoost是GBDT的高效实现，本文用一个简单的例子说明GBDT工作方式，尽量不堆公式。"
date: 2017-04-01 20:00:00
---

给定模型GBDT，记为G。给定训练集，记为D。训练集如下(假设为用户一年内的统计数据)：

|用户ID|天猫购物金额|在百度知道提问次数|年龄|
|--|:---:|:---:|---:|
|A|200|20|14|
|B|300|18|16|
|C|1500|10|20|
|D|800|8|24|
|E|3000|28|26|

给定测试集，记为T。测试集如下(假设同训练集)：

|用户ID|天猫购物金额|在百度知道提问次数|年龄|
|--|:---:|:---:|---:|
|F|4000|2|?|
|H|100|32|?|

此处声明：数据均为假设数据，其实我也不了解不同年龄段的同学们的消费和提问情况。

问题定义如下：

    在给定模型G和训练集D的前提下，训练模型G，利用训练后的模型G预测测试集T的年龄。

首先连续数据离散化，对D和T的离散化结果如下(购物金额阈值=1000，提问次数阈值=20)：

训练集:

|用户ID|购物金额<=1000|提问次数 <= 20|年龄|
|--|:---:|:---:|---:|
|A|1|0|14|
|B|1|1|16|
|C|0|1|20|
|D|1|1|24|
|E|0|0|26|

测试集:

|用户ID|购物金额<=1000|提问次数 <= 20|年龄|
|--|:---:|:---:|---:|
|F|0|1|?|
|H|1|0|?|

让我们先看看传统的回归树的做法:

![回归树](http://wx2.sinaimg.cn/mw690/aba7d18bgy1fe7gai1lqrj20gw08caa4.jpg)

模型训练完成，预测F和H的年龄：

依据F和H的属性，按照建立的树结构，用F和H落入的叶子节点的值作为F和H的预测年龄值，则F和H的年龄分别为20和20，虽然预测年龄相同，但是按照建立的模型，二者落入了不同的叶子节点。

让我们看看GBDT是怎样做的吧？模型G假定建立两棵树，每棵树的最大分支数为2。如下:

![GBDT](http://wx4.sinaimg.cn/mw690/aba7d18bgy1fe7gxvo95ej20cq0dc3yn.jpg)

【修正】红色残差值应为: B=-7/3,C=-10/3,D=17/3,A=-7/2,E=7/2

模型训练完成，预测F和H的年龄：

对于F:
    购物金额>1000，按照第一棵树，预测年龄为23，记为F_AGE_1 = 23
    提问次数<=20，按照第二棵树，预测年龄为1/3，记为F_AGE_2 = 1/3

故F的预测年龄为：F_AGE = F_AGE_1 + F_AGE_2 = 23 + 1/3 = 23.33

对于H，预测方法同F。

对于上述的例子，可以帮助我们建立GBDT的直觉，关于GBDT的理论部分，还有很多有意思的内容可以进一步的挖掘。

参考:

1.[GBDT(MART)迭代决策树入门教程](http://blog.csdn.net/w28971023/article/details/8240756)

文中的例子是对参考文章中例子的一般改进，使得数据看起来不那么完美才接近真实，同时给出了连续数据离散化的过程，这样一个标准的训练过程就有了。参考文献中在回归树部分使用了不同的属性，给出了作者的一个对比想法，作者同时提出的几个问题也是帮助我们建立直觉的很好的方式，强烈推荐这篇文章。






    