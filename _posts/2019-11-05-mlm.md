---
layout: post
title: "[NLP]用Masked Language Model搞事情"
excerpt: "这篇博客是根据个人在组里的技术分享整理而来，主要梳理近期围绕MLM在应用层上的一些工作。比如，可以用于情感迁移，中文纠错，知识发现等。"
date: 2019-11-05 10:20:00
mathjax: true
---

### 理论层

#### 一.完型填空

维基百科对完型填空任务的解释是：

>A cloze test (also cloze deletion test) is an exercise, test, or assessment consisting of a portion of language with certain items, words, or signs removed (cloze text), where the participant is asked to replace the missing language item. … The exercise was first described by W.L. Taylor in 1953.”从上述定义可以看到，该项任务从1953年已经开始，延续至今。对于国内同学而言，中文和英文两个语种的学习几乎伴随整个学生生涯。这里的完形填空可以认为就是四六级英语考试中的完型填空。该任务可以认为是下述工作的起源。

一直以来的一个不太成熟的想法：小学六年级的语文试卷或者英文试卷可以为NLP任务的设计带来一些很有启发性的思考。不妨随便找一张试卷看一看，我就尝试过。#### 二.Word2Vec

大概13/14年的工作，在个人心中地位崇高。个人认为该方法是一种对序列进行有效量化表达的工具。其中两个典型的范式CBOW和Skip-Gram。其中CBOW是给定中心词前后的词，预测中心词。Skip-Gram在范式上相反。从直观上讲，BERT蕴含了两种范式，原因如下：

给定原始输入句子，如下：

	习大大 参加 进博会 。

第一种MASK策略如下(类比CBOW)：

	习大大 [MASK] 进博会 。

第二种MASK策略如下(类比Skip-Gram)：

	[MASK] 参加 [MASK] 。

原始论文：《Efficient Estimation of Word Representations in Vector Space》

#### 三.Data Noising

ICLR2017的文章《Data Noising As Smoothing in Neural Network Language Models》提出了一些数据加噪的方式，如下：

unigram noising：随机替换

blank noising：用'_'来替换

对于blank noising来说，大概是MASK策略最早的雏形了，因为在这里第一次用显式的符号来占位。对比Word2Vec，是一种不同的实现。因此，可能对于BERT流来说，有另外的学习方式哦。不过，由于BERT基于MLM做训练，本质上是自编码的范式，那么怎么实现对齐就是需要考虑的问题了。

#### 四.BERT

BERT的训练包含两个任务，第一是Masked LM；第二是下一句预测。在第一个任务上，对于给定一句话，选择15%要处理的Token，对于15%要处理的Token，80%的Token用[MASK]替换，10%随机替换，10%保持不变。

对于fine-tune而言，近期的一些工作，经验性表明不是特别需要。对于要处理的15%的Token，其中的20%是为了保证训练和测试的一致性。

#### 五.ELECTRA

该工作是ICLR2020正在Review的工作，《ELECTRA: Pre-Training Text Encoders as Discriminators Rather Than Generators》，作者是Kevin Clark, Thang Luong, Quoc Le, Chris Manning.

[ICLR的Review地址](https://openreview.net/forum?id=r1xMH1BtvB)

其实，在BERT之前，他们的工作《Semi-Supervised Sequence Modeling with Cross-View Training》也是非常值得思考的。

这篇文章的Motivation在于希望找到更加高效的预训练语言模型的方式，包括样本高效，计算/参数高效。文章认为在MLM训练阶段，只计算15%的Token的损失函数，是一种样本不高效的做法。计算高效是指给定FLOP，模型大小一致，数据规模一致的前提下，在任务测试集上的评测指标要高。FLOP作为评估的指标，也是文章的卖点之一，以上只是个人粗浅的理解，不保证正确。

整体上的模型结构如下：

![img](https://wx1.sinaimg.cn/mw690/aba7d18bly1g8mzvi5ts2j20t507e3zs.jpg)

训练时，生成器和判别器同时训练；测试时只要判别器。那么，在该范式下，数据的构造如下：

![data](https://wx2.sinaimg.cn/mw690/aba7d18bly1g8mzv7fwshj20oc074aaz.jpg)

最后，模型的性能表现如下，红框表示最优和最差结果的对比。

![performance](https://wx4.sinaimg.cn/mw690/aba7d18bly1g8mzvdteeej20v80gp40z.jpg)

该工作是Manning在前两天的北京智源大会上讲的，对该工作的一些分析，可以参考[知乎文章](https://zhuanlan.zhihu.com/p/89763176)，因为这并不是这篇博客的目的。整体上，实验做的算是比较扎实。

### 应用层

#### 一.风格迁移

《Mask and Inﬁll: Applying Masked Language Model to Sentiment Transfer》,IJCAI 2019的工作，是非并行情感迁移任务下的SOTA，整体思路比较简单，如下：

![img_0](https://wx4.sinaimg.cn/mw690/aba7d18bly1g8n075h5ypj20f40ccdhw.jpg)

从个人的实现来看，Mask阶段做不好对后续的工作影响非常大。值得注意的点包括：

第一：如何将target sentiment的信号输入给模型？

第二：如何将预训练情感分类器的feedback传递给Infill阶段的MLM?或者，这种feedback信息是必要的吗？

#### 二.拼写纠错

爱奇艺的工作，《FASPell: A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based on DAE-Decoder Paradigm》，整体工作中规中矩，不过，作为后处理的CSD模块做了很多工作。主要考虑三个维度的信息：置信度，字音相似性和字形相似性。

![img_2](https://wx2.sinaimg.cn/mw690/aba7d18bly1g8n07dp2pmj20gb0jotb8.jpg)

#### 三.知识发现

《Language Models as Knowledge Bases?》这篇主要讨论一个有意思的观点：预训练的语言模型有没有可能已经存储了很多结构化的知识信息？那么对该知识的提取比较简单，整体上是一个类似于MLM的方式。

![img_3](https://wx3.sinaimg.cn/mw690/aba7d18bly1g8n07ho8kij20km0fugoa.jpg)

#### 四.模型蒸馏

基于MLM的蒸馏，《Pseudolikelihood Reranking with Masked Language Models》。

![img_4](https://wx3.sinaimg.cn/mw690/aba7d18bly1g8n07lj4kqj20y60fqwfr.jpg)

#### 五.机器翻译

EMNLP2019的工作，《Mask-Predict: Parallel Decoding of Conditional Masked Language Models》，主要讨论MLM用于non-autoregressive的机器翻译任务。整体上将decoding过程建模为一个迭代过程，第一步对所有Token全部MASK掉，第二步，对Token的对应输出按照置信度排个序，**选择置信度较低的Token，MASK后**作为第二个迭代过程的输入，**置信度高的输出**直接作为第二个迭代过程的输入，依次类推。
![img_5](https://wx3.sinaimg.cn/mw690/aba7d18bly1g8n079utbij20xf05k763.jpg)

### 总结

综合上述的几个工作，形成的几个结论如下：

（1）Masked LM是预训练语言模型的一个方式，但是不是唯一的方式。BERT的embedding分为contextual embedding和word embedding两种，多数时候在下游任务fine-tune的时候，使用的是前者。但是很难说，contextual embedding是好的，还是不好的。因为contextual embedding会遇到[一字多义问题](https://towardsdatascience.com/visualisation-of-embedding-relations-word2vec-bert-64d695b7f36)。其实，ELECTRA就是提供了一种细粒度modeling的方式。

(2)MLM通常要结合前处理步骤和后处理步骤。比如在拼写纠错中，是配合一个后处理操作实现；在一些场景下，使用前处理的操作可能是更加合理的一个方式，这种方式通常分为两个阶段：召回+排序。

（3）以语言模型为代表的自监督学习，潜力很大。无论是通过auto-regressive的方式，还是通过denoising autoencoder的方式，值得进一步探索。

最后，附上完整PPT下载地址。

链接: https://pan.baidu.com/s/1clOhcnxWA6zln3GB_RqGXQ

提取码: smte








